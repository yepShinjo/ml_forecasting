{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6c57f0f",
   "metadata": {},
   "source": [
    "# the_dogs_paw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0671ebc2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install pandas\n",
    "!pip install pystan==2.19.1.1\n",
    "!pip install prophet\n",
    "!pip install boto3\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "import boto3\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fdd7f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "print(sagemaker.get_execution_role())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66518eb4",
   "metadata": {},
   "source": [
    "## S3 Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7635ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "bucket = 'thedogspaw-small-forecast-data'  # <--- change to your S3 bucket\n",
    "\n",
    "# Connect to S3\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "def s3_get(key, local):\n",
    "    s3.download_file(bucket, key, local)\n",
    "\n",
    "# Download all required files to /tmp\n",
    "s3_get('datasets/thedogspaw_phppos_items.csv',         '/tmp/items.csv')\n",
    "s3_get('datasets/thedogspaw_phppos_locations.csv',     '/tmp/locations.csv')\n",
    "s3_get('datasets/thedogspaw_phppos_location_items.csv','/tmp/location_items.csv')\n",
    "s3_get('datasets/thedogspaw_phppos_sales.csv',         '/tmp/sales.csv')\n",
    "s3_get('datasets/thedogspaw_phppos_sales_items.csv',   '/tmp/sales_items.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1810d4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "items_df         = pd.read_csv('/tmp/items.csv')\n",
    "locations_df     = pd.read_csv('/tmp/locations.csv')\n",
    "locations_item_df= pd.read_csv('/tmp/location_items.csv')\n",
    "sales_df         = pd.read_csv('/tmp/sales.csv', parse_dates=['sale_time'])\n",
    "sales_items_df   = pd.read_csv('/tmp/sales_items.csv')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 50)   # set to None to show all rows if needed\n",
    "\n",
    "display(items_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a509d38c",
   "metadata": {},
   "source": [
    "## Preprocess sales.csv, items.csv and sale_items.csv NaN columns to Str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644e79ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "sales_cols_as_str = [5, 29, 37, 38, 40, 41, 53, 54, 63]  # zero-based columns\n",
    "\n",
    "sales_df = pd.read_csv(\n",
    "    '/tmp/sales.csv',\n",
    "    parse_dates=['sale_time'],\n",
    "    dtype={col: str for col in sales_cols_as_str},\n",
    "    low_memory=False\n",
    ")\n",
    "items_df = pd.read_csv(\n",
    "    '/tmp/items.csv',\n",
    "    dtype={56: str, 74: str},  # or use actual column names\n",
    "    low_memory=False\n",
    ")\n",
    "sales_items_df = pd.read_csv(\n",
    "    '/tmp/sales_items.csv',\n",
    "    dtype={6: str, 23: str},   # or use names!\n",
    "    low_memory=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377c6346",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035b3195",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicate item_ids\n",
    "duplicates = items_df[items_df.duplicated('item_id', keep=False)]\n",
    "print(f\"Found {duplicates['item_id'].nunique()} duplicated item_ids.\")\n",
    "print(duplicates.sort_values('item_id'))\n",
    "\n",
    "# Check for duplicate item names\n",
    "name_dupes = items_df[items_df.duplicated('name', keep=False)]\n",
    "print(f\"Found {name_dupes['name'].nunique()} duplicated item names.\")\n",
    "print(name_dupes.sort_values('name'))\n",
    "\n",
    "# See name duplicate counts\n",
    "name_counts = items_df['name'].value_counts()\n",
    "dupe_names = name_counts[name_counts > 1].index.tolist()\n",
    "print(f\"Found {len(dupe_names)} item names with duplicates.\")\n",
    "print(dupe_names[:20])\n",
    "\n",
    "# See all items with duplicate names\n",
    "dupes = items_df[items_df['name'].isin(dupe_names)].sort_values('name')\n",
    "print(dupes[['item_id', 'name', 'category_id', 'supplier_id']].head(30))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126f464f",
   "metadata": {},
   "source": [
    "## Merge Sales Items with Sales (Add date/store info to item sales) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da2eba78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "item_sales = sales_items_df.merge(\n",
    "    sales_df[['sale_id', 'sale_time', 'location_id']],\n",
    "    on='sale_id', how='left'\n",
    ")\n",
    "item_sales['date'] = pd.to_datetime(item_sales['sale_time']).dt.date\n",
    "print(item_sales[['sale_id', 'item_id', 'location_id', 'date', 'quantity_purchased']].tail(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ea3de7",
   "metadata": {},
   "source": [
    "## Aggregate Daily Sales Per Item Per Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013eecf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "daily_item_sales = (\n",
    "    item_sales.groupby(['location_id', 'item_id', 'date'])['quantity_purchased']\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .sort_values(['location_id', 'item_id', 'date'])\n",
    ")\n",
    "print(daily_item_sales.tail(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac464e8b",
   "metadata": {},
   "source": [
    "## Calculate enough history (have a minimum 20 total sales day after filtering) for forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3049d65d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "item_day_counts = (\n",
    "    recent_daily_item_sales.groupby(['location_id', 'item_id'])['date']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'date': 'num_days_with_sales'})\n",
    ")\n",
    "item_day_counts['enough_history'] = item_day_counts['num_days_with_sales'] >= 20\n",
    "print(item_day_counts.head(10))\n",
    "print(f\"Forecastable items: {item_day_counts['enough_history'].sum()} / {len(item_day_counts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89672a87",
   "metadata": {},
   "source": [
    "## The main shit. Forecast / fallback per item per store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdcbe16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "lead_time_days = 7\n",
    "z = 1.65\n",
    "results = []\n",
    "\n",
    "for _, row in tqdm(item_day_counts.iterrows(), total=len(item_day_counts)):\n",
    "    loc = row['location_id']\n",
    "    item = row['item_id']\n",
    "    enough = row['enough_history']\n",
    "\n",
    "    item_sales_history = daily_item_sales[\n",
    "        (daily_item_sales['location_id'] == loc) &\n",
    "        (daily_item_sales['item_id'] == item)\n",
    "    ].copy()\n",
    "    item_sales_history = item_sales_history.rename(columns={'date': 'ds', 'quantity_purchased': 'y'})\n",
    "    item_sales_history['ds'] = pd.to_datetime(item_sales_history['ds'])\n",
    "\n",
    "    # Apply 12-month cutoff in the loop, to match recent_daily_item_sales\n",
    "    cutoff_in_loop = item_sales_history['ds'].max() - pd.DateOffset(months=12)\n",
    "    item_sales_history = item_sales_history[item_sales_history['ds'] >= cutoff_in_loop]\n",
    "\n",
    "    reorder_level = None\n",
    "    replenish_level = None\n",
    "\n",
    "    if enough and len(item_sales_history) >= 20:\n",
    "        try:\n",
    "            m = Prophet(daily_seasonality=True)\n",
    "            m.fit(item_sales_history)\n",
    "            future = m.make_future_dataframe(periods=lead_time_days)\n",
    "            forecast = m.predict(future)\n",
    "            lead_forecast = forecast.tail(lead_time_days)\n",
    "            demand_lt = lead_forecast['yhat'].sum()\n",
    "            sigma_lt = (lead_forecast['yhat_upper'].sum() - lead_forecast['yhat_lower'].sum()) / 3.29\n",
    "            safety_stock = z * sigma_lt\n",
    "            reorder_level = int(np.round(demand_lt + safety_stock))\n",
    "            replenish_level = int(np.round(reorder_level + demand_lt))\n",
    "        except Exception as e:\n",
    "            last_week = item_sales_history.sort_values('ds').tail(7)\n",
    "            avg_daily = last_week['y'].mean() if len(last_week) else 1\n",
    "            demand_lt = avg_daily * lead_time_days\n",
    "            reorder_level = int(np.round(demand_lt))\n",
    "            replenish_level = int(np.round(demand_lt * 2))\n",
    "    else:\n",
    "        last_week = item_sales_history.sort_values('ds').tail(7)\n",
    "        avg_daily = last_week['y'].mean() if len(last_week) else 1\n",
    "        demand_lt = avg_daily * lead_time_days\n",
    "        reorder_level = int(np.round(demand_lt))\n",
    "        replenish_level = int(np.round(demand_lt * 2))\n",
    "\n",
    "    results.append({\n",
    "        'location_id': loc,\n",
    "        'item_id': item,\n",
    "        'reorder_level': reorder_level,\n",
    "        'replenish_level': replenish_level,\n",
    "        'enough_history': enough\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df.head(10))\n",
    "print(\"-----------------\")\n",
    "print(results_df.tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcafcdb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Merge item names\n",
    "merged_results = results_df.merge(items_df[['item_id', 'name']], on='item_id', how='left')\n",
    "\n",
    "# Last sale date for each (location, item)\n",
    "last_sale_dates = (\n",
    "    recent_daily_item_sales.groupby(['location_id', 'item_id'])['date']\n",
    "    .max()\n",
    "    .reset_index()\n",
    "    .rename(columns={'date': 'last_sale_date'})\n",
    ")\n",
    "\n",
    "merged_results = merged_results.merge(last_sale_dates, on=['location_id', 'item_id'], how='left')\n",
    "\n",
    "print(merged_results[['location_id', 'item_id', 'name', 'reorder_level', 'replenish_level', 'last_sale_date']].head(20))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f22957",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results_key = 'results/thedogspaw_reorder_replenish_results.csv'\n",
    "merged_results.to_csv('/tmp/thedogspaw_reorder_replenish_results.csv', index=False)\n",
    "s3.upload_file('/tmp/thedogspaw_reorder_replenish_results.csv', bucket, results_key)\n",
    "os.remove('/tmp/thedogspaw_reorder_replenish_results.csv')\n",
    "print(f\"✅ Reorder & replenish results written to s3://{bucket}/{results_key}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
